\section{Preliminaries}

In this section, we review today's HDFS-based column stores in which wide tables are stored, and how queries are executed on top of these wide tables. Besides, we point out existing problems and new  challenges.

\subsection{Wide Table Layout in HDFS}

Distributed joins on HDFS are very expensive.
In HDFS environment, data is partitioned into blocks and distributed in the cluster. 
Physical location of block is controlled by HDFS and transparent to computation on top of HDFS.
With this limitation, data analytic systems on HDFS are not able to co-locate blocks from related tables.
So that joins on HDFS are much more expensive than data scans due to that they shuffle large amount of data through the network, consume large amount of memory and dump large amount of intermediate data into disks.

So that a major motivation of using wide tables on HDFS is saving join cost.
There are research efforts~\cite{WideTable:paper} on assembling normalized narrow tables into wide tables to improve query performance.
In real data analytic products such as that in Microsoft Bing~\cite{ColumnOrdering}, wide tables are widely used in data analysis in HDFS environment.

Modern analytical systems have primarily opted for column stores, such as ORC File~\cite{ORCMainPage}, Parquet~\cite{parquetMainPage} and CarbonData~\cite{CarbonDataMainPage}.
This is also true in wide tables.
Column store in wide tables is good for I/O performance and makes data redundancy problem in wide tables become lightweight in two ways: First, many queries only access a small subset of the large number of columns in wide tables; second, column store facilliates data encoding and compression greatly.

In column store, a table is horizontally partitioned to scale out and to leverage multi-machine parallelism.
Each table partition, a.k.a. a \textit{row group}, is processed as a data split individually by mapper tasks.
Inside row groups, data in the same column are serialized and encoded together as a \textit{column chunk}.
By default, a HDFS block contains a row group, and a mapper task process a HDFS block as a \textit{data split}, and column chunks inside the same row group are physically aligned as their nature order specified in table schema.

We've observed that in wide tables, some queries are "large" -- they read a large number of columns in tables, while most queries are "small" -- they read a small number of columns in tables.
For small queries, a larger HDFS block, containing more row groups, is better for query performance.
In this way, one mapper task can read more row groups inside one block, thus reducing the number of tasks and further reducing overall task initial and scheduling overheads.


As for optimizing data reading performance during query execution in wide tables, we found that column order matters when organizing a row group~\cite{ColumnOrdering}.
By column ordering, we reorder the physical storage of columns inside a row group, so that frequently co-accessed columns are closer physically, which reduces seek latency -- the root cause of data reading latency -- greatly.
Furthermore, we find that compared with column ordering, column chunk ordering is a better way in HDFS blocks containing multiple row groups.
By column chunk ordering, we reorder physical storages of column chunks inside the whole block, so that frequently co-accessed column chunks are closer physically and different column chunks belonging to the same column may be relocated together.
After column chunk ordering, small queries can read column chunks of more row groups sequentially, while large queries remains unaffected.

Each mapper task processes one data split, which is default to be one HDFS block.
In order to cover both the needs of small and large queries, the data split size should be dynamic.
By being dynamic, small and large queries are treated equally and their individual mapper tasks read almost the same amount of data into memory.
Thus, small queries can get bigger data split sizes and large queries  get smaller data split sizes.

To achieve these, we face some major challenges:
\begin{itemize}
	\item \textit{Workload modeling.} We need to model workload first and then order column chunks accordingly.
	\item \textit{Column chunk ordering.} First, we need an time-efficient algorithm; second, when ordering, we need tradeoff between data reading performance and tuple reconstruction cost.
	\item \textit{Dynamic Data Splitting.} Need consider task parallelism.
\end{itemize}

%By efficient data encoding and compression provided in popular HDFS column stores such as ORC~\cite{ORCMainPage}, Parquet~\cite{parquetMainPage} and CarbonData~\cite{CarbonDataMainPage}, data redundancy in wide tables is not a major problem and the I/O performance is largely improved.
%It is much more efficient than storing data in a set of normalized tables and joining them during query runtime.

\subsection{Cost Models}

In Pixels, we focus on the optimization of data reading cost, which mainly comes from the first map stage of query execution in a MapReduce-like system, and takes a majority of the end-to-end latency for many batch queries~\cite{ColumnOrdering}. Basically, the reading cost of a map task (we assume one row group is read by a map task, although the model can be easily adapted to support multiple row groups) includes three major parts:
\begin{itemize}
	\item \textit{Constant overhead}. It includes the cost of task scheduling, metadata parsing, garbage collection, and the initial seek cost to read a row group.
	
	\item \textit{Sequential reading cost}. Given the access pattern $AP=\{c_{q,1},c_{q,2},\dots,c_{q,m}\}$ of a query $q$, the sequential reading cost on a row group is $SeqRead(q)=\sum_{i=1}^{m}size(c_{q,i})/b$, where $b$ is the sequential read bandwidth of the disk, $c_{q,i}$ is the $i^{th}$ column that is required by $q$.
	
	\item \textit{Seek cost}. Given a column order $S=\{c_1, c_2,\dots,c_n\}$, the column access pattern $AP=\{c_{q,1},c_{q,2},\dots,c_{q,m}\}$ of query $q$, the seek cost of a row group is \\$Seek(q)=\sum_{i=1}^{m-1}f(dist(c_{q,i},c_{q,i+1}))$, where $dist$ is the distance in bytes between two data items in a file, and $f$ is the seek cost function built by \textit{Seek Evaluator} in Section \ref{subsec:seekEvaluator}.
\end{itemize}

\begin{Definition}[Query reading cost]\label{equ:qcost}
	Given a query $q$, a wide table of $N$ row groups, the reading cost of $q$ is
	\begin{equation}
	Cost(q) = N\times(\epsilon+SeqRead(q)+Seek(q))
	\label{equ:querycost}
	\end{equation}
\end{Definition}

where $\epsilon$ is the constant overhead of reading a row group.
We assume that the reading cost of each row group is the same.
The reading cost of the whole workload is then modelled as:

\begin{Definition}[Workload reading cost]\label{equ:wcost}
	Given a weight $w_q$ for each query $q\in Q$, the reading cost of a workload $Q$ is
	\begin{equation}
	Cost(Q) = \sum_{q\in Q}(w_q\times Cost(q))
	\label{equ:workloadcost}
	\end{equation}
\end{Definition}

The weight $w_q$ implies the frequency/importance of the query $q$. We apply the workload reading cost as the target to be optimized for the layout optimization component.
